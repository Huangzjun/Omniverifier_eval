# T2I-ReasonBench evaluation config
# Aligned with the official protocol: https://github.com/KaiyueSun98/T2I-ReasonBench
benchmark: "t2i_reasonbench"

data:
  prompts_dir: "data/t2i_reasonbench_raw/prompts"
  eval_questions_dir: "data/t2i_reasonbench_raw/deepseek_evaluatioin_qs"
  dimensions:
    - "idiom_interpretation"
    - "textual_image_design"
    - "entity_reasoning"
    - "scientific_reasoning"

evaluation:
  # Official protocol uses Qwen2.5-VL-72B-Instruct as MLLM judge
  judge_model: "Qwen/Qwen2.5-VL-72B-Instruct"
  # Two-stage: LLM generates QA pairs (pre-computed), MLLM scores 0/0.5/1
  scoring: "two_stage"
  # Per-criterion scores are 0, 0.5, or 1 (not 0-10)
  score_levels: [0, 0.5, 1]

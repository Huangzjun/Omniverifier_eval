"""T2I-ReasonBench dataset loader.

T2I-ReasonBench contains 800 prompts across 4 dimensions:
- Idiom Interpretation (200 prompts)
- Textual Image Design (200 prompts)
- Entity-Reasoning (200 prompts)
- Scientific-Reasoning (200 prompts)

Each prompt has associated evaluation question-criteria pairs
generated by DeepSeek, used for two-stage scoring.

Reference: https://github.com/KaiyueSun98/T2I-ReasonBench
"""
from __future__ import annotations

import json
from pathlib import Path

from .base_dataset import BaseDataset, DataSample


DIMENSION_MAP = {
    "idiom_interpretation": "Idiom Interpretation",
    "textual_image_design": "Textual Image Design",
    "entity_reasoning": "Entity Reasoning",
    "scientific_reasoning": "Scientific Reasoning",
}


class T2IReasonBenchDataset(BaseDataset):
    """Loader for T2I-ReasonBench benchmark."""

    def __init__(
        self,
        prompts_dir: str = "data/t2i_reasonbench_raw/prompts",
        eval_questions_dir: str = "data/t2i_reasonbench_raw/deepseek_evaluatioin_qs",
        dimensions: list[str] | None = None,
    ):
        super().__init__(name="t2i_reasonbench")
        self.prompts_dir = Path(prompts_dir)
        self.eval_questions_dir = Path(eval_questions_dir)
        self.dimensions = dimensions or list(DIMENSION_MAP.keys())

    def load(self) -> None:
        """Load prompts and evaluation criteria from disk."""
        self._samples = []

        for dim in self.dimensions:
            prompt_file = self.prompts_dir / f"{dim}.json"
            eval_file = self.eval_questions_dir / f"{dim}.json"

            if not prompt_file.exists():
                raise FileNotFoundError(
                    f"Prompt file not found: {prompt_file}\n"
                    f"Please clone T2I-ReasonBench: "
                    f"git clone https://github.com/KaiyueSun98/T2I-ReasonBench.git data/t2i_reasonbench_raw"
                )

            with open(prompt_file, "r") as f:
                prompts = json.load(f)

            # Load evaluation question-criteria pairs if available
            eval_data = {}
            if eval_file.exists():
                with open(eval_file, "r") as f:
                    eval_data = json.load(f)

            for idx, prompt_entry in enumerate(prompts):
                # Handle both list-of-strings and list-of-dicts formats
                if isinstance(prompt_entry, str):
                    prompt_text = prompt_entry
                    sample_id = f"{dim}_{idx:04d}"
                elif isinstance(prompt_entry, dict):
                    prompt_text = prompt_entry.get("prompt", prompt_entry.get("text", ""))
                    sample_id = prompt_entry.get("id", f"{dim}_{idx:04d}")
                else:
                    continue

                # Attach evaluation QA pairs
                metadata = {
                    "dimension_full_name": DIMENSION_MAP.get(dim, dim),
                    "index": idx,
                }
                if eval_data:
                    # eval_data is typically keyed by prompt or index
                    if isinstance(eval_data, list) and idx < len(eval_data):
                        metadata["eval_questions"] = eval_data[idx]
                    elif isinstance(eval_data, dict):
                        key = str(idx) if str(idx) in eval_data else prompt_text
                        if key in eval_data:
                            metadata["eval_questions"] = eval_data[key]

                self._samples.append(
                    DataSample(
                        id=sample_id,
                        prompt=prompt_text,
                        dimension=dim,
                        metadata=metadata,
                    )
                )

        print(f"[T2I-ReasonBench] Loaded {len(self._samples)} prompts across {len(self.dimensions)} dimensions")

    def get_image_filename(self, sample: DataSample) -> str:
        """Get expected image filename for a sample (e.g., '0001.png')."""
        idx = sample.metadata.get("index", 0)
        return f"{idx + 1:04d}.png"

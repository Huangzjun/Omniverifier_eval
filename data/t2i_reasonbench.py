"""T2I-ReasonBench dataset loader.

T2I-ReasonBench contains 800 prompts across 4 dimensions:
- Idiom Interpretation (200 prompts)
- Textual Image Design (200 prompts)
- Entity-Reasoning (200 prompts)
- Scientific-Reasoning (200 prompts)

Each prompt has associated evaluation question-criteria pairs
generated by DeepSeek, used for two-stage scoring.

Reference: https://github.com/KaiyueSun98/T2I-ReasonBench
"""
from __future__ import annotations

import json
from pathlib import Path

from .base_dataset import BaseDataset, DataSample


DIMENSION_MAP = {
    "idiom_interpretation": "Idiom Interpretation",
    "textual_image_design": "Textual Image Design",
    "entity_reasoning": "Entity Reasoning",
    "scientific_reasoning": "Scientific Reasoning",
}

# The official repo names eval JSON files differently from the dimension keys.
EVAL_FILE_MAP = {
    "idiom_interpretation": "evaluation_idiom.json",
    "textual_image_design": "evaluation_textual_image.json",
    "entity_reasoning": "evaluation_entity.json",
    "scientific_reasoning": "evaluation_scientific.json",
}


class T2IReasonBenchDataset(BaseDataset):
    """Loader for T2I-ReasonBench benchmark."""

    def __init__(
        self,
        prompts_dir: str = "data/t2i_reasonbench_raw/prompts",
        eval_questions_dir: str = "data/t2i_reasonbench_raw/deepseek_evaluation_qs",
        dimensions: list[str] | None = None,
    ):
        super().__init__(name="t2i_reasonbench")
        self.prompts_dir = Path(prompts_dir)
        self.eval_questions_dir = Path(eval_questions_dir)
        self.dimensions = dimensions or list(DIMENSION_MAP.keys())

    def load(self) -> None:
        """Load prompts and evaluation criteria from disk."""
        self._samples = []

        for dim in self.dimensions:
            prompt_file = self.prompts_dir / f"{dim}.json"

            # Eval QA files use a different naming convention in the official repo
            eval_filename = EVAL_FILE_MAP.get(dim, f"{dim}.json")
            eval_file = self.eval_questions_dir / eval_filename
            # Fallback: try the dimension-key name directly
            if not eval_file.exists():
                eval_file = self.eval_questions_dir / f"{dim}.json"

            if not prompt_file.exists():
                raise FileNotFoundError(
                    f"Prompt file not found: {prompt_file}\n"
                    f"Please clone T2I-ReasonBench: "
                    f"git clone https://github.com/KaiyueSun98/T2I-ReasonBench.git data/t2i_reasonbench_raw"
                )

            with open(prompt_file, "r") as f:
                prompts = json.load(f)

            eval_data: list | dict = {}
            if eval_file.exists():
                with open(eval_file, "r") as f:
                    eval_data = json.load(f)

            for idx, prompt_entry in enumerate(prompts):
                if isinstance(prompt_entry, str):
                    prompt_text = prompt_entry
                elif isinstance(prompt_entry, dict):
                    prompt_text = prompt_entry.get("prompt", prompt_entry.get("text", ""))
                else:
                    continue
                sample_id = f"{dim}_{idx:04d}"

                metadata: dict = {
                    "dimension_key": dim,
                    "dimension_full_name": DIMENSION_MAP.get(dim, dim),
                    "index": idx,
                }
                if eval_data:
                    if isinstance(eval_data, list) and idx < len(eval_data):
                        metadata["eval_questions"] = eval_data[idx]
                    elif isinstance(eval_data, dict):
                        key = str(idx) if str(idx) in eval_data else prompt_text
                        if key in eval_data:
                            metadata["eval_questions"] = eval_data[key]

                self._samples.append(
                    DataSample(
                        id=sample_id,
                        prompt=prompt_text,
                        dimension=dim,
                        metadata=metadata,
                    )
                )

        print(f"[T2I-ReasonBench] Loaded {len(self._samples)} prompts across {len(self.dimensions)} dimensions")

    def get_image_filename(self, sample: DataSample) -> str:
        """Get expected image filename for a sample (e.g., '0001.png')."""
        idx = sample.metadata.get("index", 0)
        return f"{idx + 1:04d}.png"
